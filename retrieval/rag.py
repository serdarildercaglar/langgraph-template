"""
RAG pipeline with dense vectors, FTS (BM25), and hybrid search.
"""

import json
import uuid
from pathlib import Path
from typing import Any, Literal

from retrieval.embeddings import EmbeddingService, get_embedding_service
from retrieval.milvus_client import AsyncMilvusClient, build_schema, get_milvus_client


class RAGPipeline:
    """RAG pipeline for document ingestion and retrieval."""

    def __init__(
        self,
        collection_name: str,
        embedding_service: EmbeddingService | None = None,
        milvus_client: AsyncMilvusClient | None = None,
    ):
        self.collection_name = collection_name
        self._embedding_service = embedding_service
        self._milvus_client = milvus_client

    @property
    def embeddings(self) -> EmbeddingService:
        if self._embedding_service is None:
            self._embedding_service = get_embedding_service()
        return self._embedding_service

    @property
    def milvus(self) -> AsyncMilvusClient:
        if self._milvus_client is None:
            self._milvus_client = get_milvus_client()
        return self._milvus_client

    async def initialize(
        self,
        dense_fields: list[dict[str, Any]],
        fts_fields: list[dict[str, Any]] | None = None,
        partition_key: str | None = None,
    ) -> None:
        """
        Create collection with schema.

        Args:
            dense_fields: Dense vector configs (name, source, dim, metric)
            fts_fields: FTS/BM25 configs (name, source, analyzer)
            partition_key: Field for partition key
        """
        default_dim = self.embeddings.dimension
        for df in dense_fields:
            if "dim" not in df:
                df["dim"] = default_dim

        schema, index_params = build_schema(dense_fields, fts_fields, partition_key)
        await self.milvus.create_collection(
            collection_name=self.collection_name,
            schema=schema,
            index_params=index_params,
        )

    async def ingest(
        self,
        documents: list[dict[str, Any]],
        dense_fields: list[dict[str, Any]],
        fts_fields: list[dict[str, Any]] | None = None,
        partition_key: str | None = None,
        required_fields: list[str] | None = None,
    ) -> dict[str, Any]:
        """
        Ingest documents with dense embeddings. FTS fields are auto-indexed by Milvus.

        Args:
            documents: List of documents
            dense_fields: Dense vector configs (name, source)
            fts_fields: FTS configs (name, source) - text copied, Milvus tokenizes
            partition_key: Partition key field name
            required_fields: Fields that must be non-empty (default: ["content"])
        """
        fts_fields = fts_fields or []
        required_fields = required_fields or ["content"]

        # Validate required fields
        for i, doc in enumerate(documents):
            for field in required_fields:
                value = doc.get(field)
                if not value or (isinstance(value, str) and not value.strip()):
                    raise ValueError(f"Document {i} missing required field: {field}")

        # Collect source fields for dense embeddings
        dense_sources = {df["name"]: df["source"] for df in dense_fields}

        # Collect source fields for FTS (just need to copy text)
        fts_sources = {ff["source"] for ff in fts_fields}

        # All source fields
        all_sources = set(dense_sources.values()) | fts_sources

        # Embed dense fields (handle empty/None by using empty string placeholder)
        embeddings_by_field: dict[str, list[list[float]]] = {}
        for vec_name, source in dense_sources.items():
            # Use empty string for None/empty values
            texts = [doc.get(source) or "" for doc in documents]
            embeddings_by_field[vec_name] = await self.embeddings.embed_batch(texts)

        # Build records
        prepared = []
        for i, doc in enumerate(documents):
            record = {"id": doc.get("id") or str(uuid.uuid4())}

            # Copy source text fields (use empty string for None/empty)
            for source in all_sources:
                value = doc.get(source)
                record[source] = value if value else ""

            # Add partition key
            if partition_key and partition_key in doc:
                record[partition_key] = doc[partition_key]

            # Add dense vectors
            for vec_name in dense_sources:
                record[vec_name] = embeddings_by_field[vec_name][i]

            # FTS sparse vectors are auto-generated by Milvus BM25 function

            prepared.append(record)

        return await self.milvus.insert_batch(self.collection_name, prepared)

    async def search_dense(
        self,
        query: str,
        vector_field: str,
        top_k: int = 5,
        filter_expr: str | None = None,
    ) -> list[dict[str, Any]]:
        """Semantic search on dense vector field."""
        query_embedding = await self.embeddings.embed(query)

        results = await self.milvus.search(
            collection_name=self.collection_name,
            query_vectors=[query_embedding],
            anns_field=vector_field,
            top_k=top_k,
            filter_expr=filter_expr,
        )

        return results[0] if results else []

    async def search_fts(
        self,
        query: str,
        sparse_field: str,
        top_k: int = 5,
        filter_expr: str | None = None,
    ) -> list[dict[str, Any]]:
        """Full-text search on sparse/BM25 field."""
        results = await self.milvus.search(
            collection_name=self.collection_name,
            query_vectors=[query],  # Raw text for BM25
            anns_field=sparse_field,
            top_k=top_k,
            filter_expr=filter_expr,
        )

        return results[0] if results else []

    async def search_hybrid(
        self,
        query: str,
        dense_fields: list[str],
        fts_fields: list[str] | None = None,
        top_k: int = 5,
        filter_expr: str | None = None,
        rerank: Literal["rrf", "weighted"] = "rrf",
        weights: list[float] | None = None,
    ) -> list[dict[str, Any]]:
        """
        Hybrid search across dense and FTS fields.

        Args:
            query: Search query
            dense_fields: Dense vector fields
            fts_fields: FTS/BM25 sparse fields
            top_k: Number of results
            filter_expr: Pre-filter
            rerank: "rrf" or "weighted"
            weights: Weights for weighted rerank
        """
        fts_fields = fts_fields or []
        query_embedding = await self.embeddings.embed(query)

        requests = []

        # Dense requests
        for field in dense_fields:
            requests.append({
                "data": [query_embedding],
                "anns_field": field,
                "params": {"metric_type": "COSINE"},
            })

        # FTS requests
        for field in fts_fields:
            requests.append({
                "data": [query],  # Raw text
                "anns_field": field,
                "params": {"metric_type": "BM25"},
            })

        return await self.milvus.hybrid_search(
            collection_name=self.collection_name,
            requests=requests,
            top_k=top_k,
            filter_expr=filter_expr,
            rerank=rerank,
            weights=weights,
        )

    async def delete(
        self,
        ids: list[str] | None = None,
        filter_expr: str | None = None,
    ) -> dict[str, Any]:
        """Delete documents."""
        return await self.milvus.delete(self.collection_name, ids=ids, filter_expr=filter_expr)


async def load_from_json(json_path: str | Path) -> dict[str, RAGPipeline]:
    """
    Load collections from JSON config and ingest documents.

    Args:
        json_path: Path to collections JSON file

    Returns:
        Dict of collection_name -> RAGPipeline
    """
    with open(json_path) as f:
        config = json.load(f)

    pipelines: dict[str, RAGPipeline] = {}

    for coll in config:
        name = coll["collection"]
        partition_key = coll.get("partition_key")
        dense_fields = coll.get("fields", {}).get("dense", [])
        fts_fields = coll.get("fields", {}).get("fts", [])
        documents = coll.get("documents", [])
        required_fields = coll.get("required_fields")  # Optional, defaults to ["content"]

        pipeline = RAGPipeline(collection_name=name)

        # Create collection
        await pipeline.initialize(
            dense_fields=dense_fields,
            fts_fields=fts_fields,
            partition_key=partition_key,
        )

        # Ingest documents
        if documents:
            await pipeline.ingest(
                documents=documents,
                dense_fields=dense_fields,
                fts_fields=fts_fields,
                partition_key=partition_key,
                required_fields=required_fields,
            )

        pipelines[name] = pipeline

    return pipelines
